{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Problème : analyser les transcriptions de vidéos afin de déterminer les différents sujets abordés dans les vidéos.\n",
    "\n",
    "Problèmes : \n",
    "* 1- comment déterminer un tags pour une vidéo?\n",
    "* 2- nos transcriptions ne sont pas labialisées. Comment labelliser de nombreuse transcriptions ? \n",
    "\n",
    "\n",
    "Solutions proposées: \n",
    "1- \n",
    "* La déterminations des sujet principaux dans un texte peut être faite avec de nombreuses techniques.\n",
    "Celles que nous avons retenues sont :\n",
    "* Latent Derichelet technique []\n",
    "* TF-IDF\n",
    "L'algorithme du TF-IDF identifie les mots les plus important dans le texte en fonction de leur fréquence.\n",
    "Dans le texte. Contrairement au LDA il ne permet pas d'associer les mots à un topic.\n",
    "\n",
    "Le LDA est expliqué dans ce Kernel nous avons réalisé un petit exemple sur un petit set de 3 transcriptions de \n",
    "vidéos prise sur le youtube easy-movie.\n",
    "\n",
    "L'algorithme du LDA permet pour un K donné de déterminer une K topics dans un texte en utilisant.\n",
    "les probabilité conditionnel. L'algorithme ne donne pas le nom des topics, il associe à chaque topic.\n",
    "une les mots qui ont la plus forte probabilité de générer le topic.\n",
    "\n",
    "Avec cette algorithme, nous pourrons obtenir une liste de labels pour les transcriptions\n",
    "Par la suite, nous pourrions à l'aide de cette liste de labels déterminer le thème principal\n",
    "du document. Il nous sera donc possible de labelliser avec un tag unique chaque transcription.\n",
    " \n",
    "2 - \n",
    "Afin de tagger un grand nombre de transcriptions, nous avons pensé a construire un outil de \n",
    "labélisation de texte. Cette Outils permettra à un utilisateur d'un simple clique de \n",
    "tagger les textes rapidement.\n",
    "\n",
    "* une interface graphique affichera le texte ainsi que les K (paramètre réglable) mots associé\n",
    "a la topic.\n",
    "* L'utilisateur aura alors la possibilité de tagger le texte avec des tags prédéfini ou de simplement\n",
    "créer un nouveau tag et l'associé à la transcription.\n",
    "* Une fois les textes labellisé il sera possible d'exporter un fichier CSV (ou autre format) pour\n",
    "entrainé des modèles sur les données labélisées.\n",
    "\n",
    "Afin de classifier les textes par topiques 4 modèles ont été retenu :\n",
    "\n",
    "Scikit-learn:\n",
    "* Linear-SVR\n",
    "* KNN\n",
    "* Logistic regression\n",
    "Deep-learning Pytorch :\n",
    "* Classification with attention based netwxork with BLSTM\n",
    "\n",
    "\n",
    "Ces modèles ont été choisis en fonction de la nature du problème, la classification de donner en K\n",
    "catégories et également pour leurs précisions sur ce genre de problèmes. \n",
    " Ressource : \n",
    " \n",
    " Comparaison de model :\n",
    " https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568 \n",
    " \n",
    " Attention networks:\n",
    " https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/\n",
    " https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c\n",
    "      "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition  import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd \n",
    "import pysrt\n",
    "\n",
    "class LoadData:\n",
    "    __df = pd.DataFrame()\n",
    "\n",
    "    def __init__(self, path, extension='srt'):\n",
    "        self.path = path \n",
    "        self.__files = [f for f in listdir(self.path) if (isfile(join(self.path, f)) and f.endswith(extension))]\n",
    "    \n",
    "    def files(self):\n",
    "        return self.__files\n",
    "    \n",
    "    def data(self, verbose=True):\n",
    "        textes = []\n",
    "        for file in self.__files:\n",
    "            if verbose :\n",
    "                print(f\"reading info of file {file}\")\n",
    "            texte = self.__treat_srt_file(file, verbose=verbose)\n",
    "            textes.append(texte)\n",
    "        self.__df['title'] = self.files()\n",
    "        self.__df['texte'] = textes\n",
    "        return self.__df\n",
    "        \n",
    "    def __treat_srt_file(self, file, verbose):\n",
    "        f_path = \"{}{}\".format(self.path, file)\n",
    "        print(f_path)\n",
    "        f = pysrt.open(f_path, encoding='utf-8')\n",
    "        article = \"\"\n",
    "        for t in f:\n",
    "            article += \" \" + t.text\n",
    "        print(f\"article {article}\")\n",
    "        return article\n",
    "            \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "saint-gobin.srt\nsupbiotect.srt\nevo-comment-utiliser-easy-movie.srt\nscor-simplifier-harmoniser.srt\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loader = LoadData(path='./TextFiles')\n",
    "\n",
    "for file in loader.files():\n",
    "    print(file)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "reading info of file saint-gobin.srt\n./saint-gobin.srt\narticle  [Musique] bonjour je suis séverine du service communication de saint gobain distribution bâtiment france et je vais vous présenter izimmo ville vous souhaitez communiquer avec des vidéos nous vous proposons la solution easy movie pour vous y aider simple pratique et rapide l'outil imovie permet de faire des vidéos courtes à moindre coût et de qualité comment fonctionne izimmo vient créer un nouveau projet sélectionnez votre scénario choisissez votre plan est filmée pour utiliser imovie c'est très simple il vous suffit de suivre les instructions sur l'appli imovie et de lancer l'enregistrement une fois votre film terminé il vous suffit de l'envoyer aux équipes de montage du movie 48 heures plus tard vous recevez votre vidéo montée si cette solution vous intéresse n'hésitez pas à nous contacter à bientôt [Musique]\nreading info of file supbiotect.srt\n./supbiotect.srt\narticle  bonjour je suis louis julian ancien de sup'biotech diplômé d'ingénieur en biotechnologie suite à ce biotech j'étais embauché comme directrice du sinn chez bio group une société qui faisait du coup mouchard et maintenant donc je travaille avec michel des portes était aux manettes à construire une société qui s'appelle aéro mat l'idée d'aero maths c'est de cultiver des plantes aromatiques sur les toits de paris à destination des parisiens donc directement dans le bâtiment ou dans le quartier à travers le réseau d'amap la ruche qui dit oui et d'autres réseaux de distribution on cultive sur les toits pour deux raisons d'abord parce qu'on a beaucoup de mal à trouver dans paris des herbes aromatiques fraîches et goûtu et donc l'idée d'ailleurs aux maths c'est vraiment de cultiver sur le toit pour pouvoir récolter le jour de la consommation et avoir quelque chose de très frais et la seconde raison c'est d'apporter un peu de biodiversité dans paris on apporte de la verdure et donc on va voir toute une faune qui vient sur le toit et c'est super chez aéro maths on cultive en hydroponie écologique l'idée du l'hydroponie c'est de faire pousser les plantes sans terre directement dans l'eau on devient asperges et l'eau qui contient des sels minéraux et donc les nutriments nécessaires pour la plante directement à la racine la plante va pouvoir absorber à ses minéraux et pousser l'eau ensuite va retomber dans le bac initiale ce qui à faire un circuit fermé et donc ça va permettre d'économiser 90 % d'eau par rapport à une culture en terre le second intérêt de l'hydroponie c'est que c'est une technique qui est très légère on est à moins de 100 kg par mètre carré faut savoir que les toitures terrasses parisiennes ont en moyenne 200 kg par mètre carré de portance et donc ça permet vraiment de s'adapter à tous les toits terrasses parisien les la plupart des agriculteurs urbains ont tendance à monter des bacs en terre pour cultiver sur les toits le problème de la terre c'est qu'on va très vite atteindre déportance supérieure à 7 200 kg par mètre carré on va être à 300 voire 500 kg intérêts et donc on peut pas s'installer partout contrairement à l'hydroponie qui va vraiment pouvoir s'adapter aux toitures terrasses parisiennes dans cette belle aventure sup'biotech est toujours là pour nous aider ça c'est vraiment quelque chose de top on a un gars qui va nous aider pour la com on a aussi un professeur qu'avait michel en marketing qui nous propose son aide et il ya tout un réseau qui s'est mis en place ça nous a vraiment beaucoup aidé on n'a pas ça on est entrés dans le réseau ionis 361 qui est l'incubateur aussi de ionis grâce à supiot tech donc vraiment là ça nous a beaucoup apportés pour le lancement d'aero matin après quelques conseils je donnerai aux gens qui veulent se lancer c'est vraiment ne pas lâcher et de soumettre à plusieurs parce que ça aide énormément d'être au moins deux ou trois sur un projet [Musique]\nreading info of file evo-comment-utiliser-easy-movie.srt\n./evo-comment-utiliser-easy-movie.srt\narticle  on peut y aller là le cadre il est bon voilà je crois c'est pas mal allez c'est parti pour gagner en autonomie en réactivité en rapidité on s'est doté d'une solution pour des vidéos made in eovi mcd limouzy on a suivi une formation poussée cadrage lumières sons on sait maintenant mettre en valeur vos projets et vos réussites nous irons également sur le terrain à la rencontre de nos adhérents de nos partenaires pour les mettre en boîte enfin on va les contrer préparez vous on arrive à action [Musique]\nreading info of file scor-simplifier-harmoniser.srt\n./scor-simplifier-harmoniser.srt\narticle  c'était global avec 38 bureaux implantés partout dans le monde mais halak en groupe et pas uniquement à paris nous étions à la recherche d'un outil capable d'harmoniser l'ensemble des vidéos produites par les collaborateurs du groupe avec les dinos vie on donne un cadre et une simplicité à l'ensemble de toute cette production de vidéos nous avons éliminé toutes les problématiques de charte que l'on soit maintenant à paris singapour ou à new york toutes les vidéos sont aux couleurs de score mais qu'ils imovie ont produit de plus en plus de vidéos parce que les gens sont libres de leurs mouvements ils sont plus autonomes ils peuvent se déplacer avec leur smartphone sans utiliser une grosse caméra qui peuvent filmer où ils veulent et ils se sont beaucoup plus ce format court et dynamique de vidéos correspond à la culture des réseaux sociaux que l'on essaie d'insuffler chez scor on espère continuer dans cette dynamique et cette culture de la vidéo et produire toujours plus de vidéos avec les immoler\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "                                 title  \\\n0                      saint-gobin.srt   \n1                       supbiotect.srt   \n2  evo-comment-utiliser-easy-movie.srt   \n3       scor-simplifier-harmoniser.srt   \n\n                                               texte  \n0   [Musique] bonjour je suis séverine du service...  \n1   bonjour je suis louis julian ancien de sup'bi...  \n2   on peut y aller là le cadre il est bon voilà ...  \n3   c'était global avec 38 bureaux implantés part...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>texte</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>saint-gobin.srt</td>\n      <td>[Musique] bonjour je suis séverine du service...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>supbiotect.srt</td>\n      <td>bonjour je suis louis julian ancien de sup'bi...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>evo-comment-utiliser-easy-movie.srt</td>\n      <td>on peut y aller là le cadre il est bon voilà ...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>scor-simplifier-harmoniser.srt</td>\n      <td>c'était global avec 38 bureaux implantés part...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 39
    }
   ],
   "source": [
    "df = loader.data()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CountVectorizer:\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature \n",
    "selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "* max_df : If a word appear in more than 90% of the document it will be discarded \n",
    "* min_df : If a word appear less than 2 time it will not be take in count \n",
    "\n",
    "I used spacy french stop words list. We need to remove all the french \n",
    "stop word from our token because they appear in all the document and \n",
    "can be considered as topic's words. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('fr_core_news_sm')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "import spacy.cli\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "spacy_nlp = spacy.load('fr_core_news_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "nous\npaf\nça\nmes\nla\nmince\ncelle\ntoi\nunes\nparmi\ncrac\nen\nfont\nlesquels\ntiennes\ncontre\nsein\nvingt\nquoique\ncelles\ndring\nmoi-meme\nvous\nsoi-même\nreste\nminimale\ndevers\nès\nmerci\nsuivre\nouste\ndepuis\nhurrah\nmeme\nsuivantes\naupres\nchaque\nolé\nprès\neuh\nextenso\ntreize\nanterieur\nj’\nrend\nautrement\npossessif\nquant\ntrois\ndifferent\nd’\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "spacy_stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "for i, w in enumerate(spacy_stop_words):\n",
    "    print(w)\n",
    "    if i >= 50:\n",
    "        break "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=spacy_stop_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/remy.d.w/.virtualenvs/PROROK/untitled/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n  'stop_words.' % sorted(inconsistent))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dtm = count_vectorizer.fit_transform(df['texte'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LDA use the naives bayes probability to affect each word to a \n",
    "topic and text to a topic.\n",
    "\n",
    "* each word is associated to a topic. This topic is the one that have the \n",
    "best chance to  generate the word in this document. Example the word 'formula one'\n",
    " have higher probabilty to be generated in a sport or car document that in a cooking one.\n",
    " \n",
    "* The new theme is the one that have the best chance to generate the document.\n",
    "\n",
    "* P(T | D) give the likelihood that the Doc D is affected to the Theme T  \n",
    "* P(W | T) give the likelihood that the Word W is affected to the Theme T\n",
    "\n",
    "Here we want to find the 10 topic. For each topic a list of word are choosen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n                          evaluate_every=-1, learning_decay=0.7,\n                          learning_method='batch', learning_offset=10.0,\n                          max_doc_update_iter=100, max_iter=10,\n                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n                          perp_tol=0.1, random_state=None,\n                          topic_word_prior=None, total_samples=1000000.0,\n                          verbose=0)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 44
    }
   ],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=10)\n",
    "\n",
    "LDA.fit(dtm)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    print(f\"Topic n° {i}\")\n",
    "    last_ten_words = topic.argsort()[-5:]\n",
    "    features = count_vectorizer.get_feature_names()\n",
    "    for index in last_ten_words:\n",
    "        print(features[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n\n\n\n\nTopic n° 0\noutil\nsolution\ncadre\nvidéos\nmusique\n\n\n\n\n\nTopic n° 1\nizimmo\nsimple\nsolution\nmusique\nimovie\n\n\n\n\n\nTopic n° 2\nensemble\ndynamique\nparis\nculture\nvidéos\n\n\n\n\n\nTopic n° 3\noutil\nsolution\ncadre\nvidéos\nmusique\n\n\n\n\n\nTopic n° 4\noutil\nsolution\ncadre\nvidéos\nmusique\n\n\n\n\n\nTopic n° 5\noutil\nsolution\ncadre\nvidéos\nmusique\n\n\n\n\n\nTopic n° 6\ncrois\nallez\ncadre\nmal\nmettre\n\n\n\n\n\nTopic n° 7\noutil\nsolution\ncadre\nvidéos\nmusique\n\n\n\n\n\nTopic n° 8\noutil\nsolution\ncadre\nvidéos\nmusique\n\n\n\n\n\nTopic n° 9\neau\nhydroponie\ntoits\nkg\nvraiment\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "supbiotech_vec = count_vectorizer.transform([df.iloc[0, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "saint_gobin_topics = LDA.transform(supbiotech_vec)\n",
    "\n",
    "def extractTopic(components):\n",
    "    last_ten_words = components.argsort()[-5:]\n",
    "    features = count_vectorizer.get_feature_names()\n",
    "    for index in last_ten_words:\n",
    "        print(features[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "izimmo\nsimple\nsolution\nmusique\nimovie\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "index = np.argmax(saint_gobin_topics)\n",
    "extractTopic(LDA.components_[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to add each labels array to his corresponding text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def extractTopicAndLoadInDataframe(count_vect, dataFrame, lda, nb_words=5):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param cv: countVectorizer that was applied on the documents \n",
    "    :param df: the dataframe that contains the data \n",
    "    :param lda: Latent derichlet allocation trained \n",
    "    :param nb_words: number of words by topic that you want to used\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    document_labels = []\n",
    "    for i, text in enumerate(dataFrame['texte']):\n",
    "        text_vectorized = count_vect.transform([text])\n",
    "        index = np.argmax(lda.transform(text_vectorized))\n",
    "        topic = lda.components_[index]\n",
    "        topic_word_index = topic.argsort()[-nb_words:] # get the 5 best word of the topic\n",
    "        features = count_vect.get_feature_names()\n",
    "        labels = []\n",
    "        for j in topic_word_index:\n",
    "            labels.append(features[j]) \n",
    "        document_labels.append(labels)\n",
    "    dataFrame['labels'] = document_labels\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "extractTopicAndLoadInDataframe(count_vect=count_vectorizer,dataFrame=df, lda=LDA, nb_words=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 title  \\\n0                      saint-gobin.srt   \n1                       supbiotect.srt   \n2  evo-comment-utiliser-easy-movie.srt   \n3       scor-simplifier-harmoniser.srt   \n\n                                               texte  \\\n0   [Musique] bonjour je suis séverine du service...   \n1   bonjour je suis louis julian ancien de sup'bi...   \n2   on peut y aller là le cadre il est bon voilà ...   \n3   c'était global avec 38 bureaux implantés part...   \n\n                                          labels  \n0    [izimmo, simple, solution, musique, imovie]  \n1         [eau, hydroponie, toits, kg, vraiment]  \n2             [crois, allez, cadre, mal, mettre]  \n3  [ensemble, dynamique, paris, culture, vidéos]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>texte</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>saint-gobin.srt</td>\n      <td>[Musique] bonjour je suis séverine du service...</td>\n      <td>[izimmo, simple, solution, musique, imovie]</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>supbiotect.srt</td>\n      <td>bonjour je suis louis julian ancien de sup'bi...</td>\n      <td>[eau, hydroponie, toits, kg, vraiment]</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>evo-comment-utiliser-easy-movie.srt</td>\n      <td>on peut y aller là le cadre il est bon voilà ...</td>\n      <td>[crois, allez, cadre, mal, mettre]</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>scor-simplifier-harmoniser.srt</td>\n      <td>c'était global avec 38 bureaux implantés part...</td>\n      <td>[ensemble, dynamique, paris, culture, vidéos]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 51
    }
   ],
   "source": [
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I build a pipline. This pipline contains:\n",
    " * the cleaning of the data\n",
    "   * lemming it's a normalization technique which converts high dimensional features into low dimensional features.\n",
    "       We simply identify prefix and suffix and produce the simplest version of the word. \n",
    "       For example : connect, connection, connected => have different grammatical function but for ML models we need\n",
    "       that this words be one simple word (connect) because it's can be confusing.\n",
    "       \n",
    "   * tokenizing : transform into token \n",
    "   * removing punct : It's useless for text classification \n",
    "   * removing stop words : stop words can lead to poor performance. \n",
    "  \n",
    " * Vectorize the data with CountVectorizer: Convert a collection of text documents to a matrix of token counts\n",
    " * Apply the LDA on the data : have been previously explained. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('fr_core_news_sm')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import spacy.cli\n",
    "from spacy.lang.fr import French\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "spacy_nlp = spacy.load('fr_core_news_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "parser = French()\n",
    "\n",
    "tokenizer = French()\n",
    "\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "punctuations = spacy.lang.fr.punctuation.LIST_PUNCT\n",
    "\n",
    "def easy_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param sentence: the texte we want to treat with our parser \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # creating our token object \n",
    "    easy_tokens = parser(sentence)\n",
    "    \n",
    "    # lemming  and lowering. Stripping white space \n",
    "    # if the word is not a Pronoun lower the word and strip space else return the lower word. \n",
    "    # Because  spacy replace pronouns bu \"-PRON-\" on lemming call. as we need the pronoun we can not do its lemmatization\n",
    "    \n",
    "    easy_tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower() for word in easy_tokens ]\n",
    "    \n",
    "    print(easy_tokens)\n",
    "    \n",
    "    # remove stop words \n",
    "    easy_tokens = [word  for word in easy_tokens if (word not in stop_words and word not in punctuations)]\n",
    "\n",
    "    \n",
    "    return easy_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "class PrepareText(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Take a dataframe as paremeter and fit to the data set è\n",
    "        :param X: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        remove stop word in the text and retore the text\n",
    "        return a data frame that contains all the text \n",
    "        :param X: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return [clean_text(text) for text in X]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "easy_vectorizer = CountVectorizer(tokenizer=easy_tokenizer, ngram_range=(1,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['', '[', 'musique', ']', 'bonjour', 'je', 'suis', 'séverine', 'du', 'service', 'communication', 'de', 'saint', 'gobain', 'distribution', 'bâtiment', 'france', 'et', 'je', 'vais', 'vous', 'présenter', 'izimmo', 'ville', 'vous', 'souhaitez', 'communiquer', 'avec', 'des', 'vidéos', 'nous', 'vous', 'proposons', 'la', 'solution', 'easy', 'movie', 'pour', 'vous', 'y', 'aider', 'simple', 'pratique', 'et', 'rapide', \"l'\", 'outil', 'imovie', 'permet', 'de', 'faire', 'des', 'vidéos', 'courtes', 'à', 'moindre', 'coût', 'et', 'de', 'qualité', 'comment', 'fonctionne', 'izimmo', 'vient', 'créer', 'un', 'nouveau', 'projet', 'sélectionnez', 'votre', 'scénario', 'choisissez', 'votre', 'plan', 'est', 'filmée', 'pour', 'utiliser', 'imovie', \"c'\", 'est', 'très', 'simple', 'il', 'vous', 'suffit', 'de', 'suivre', 'les', 'instructions', 'sur', \"l'\", 'appli', 'imovie', 'et', 'de', 'lancer', \"l'\", 'enregistrement', 'une', 'fois', 'votre', 'film', 'terminé', 'il', 'vous', 'suffit', 'de', \"l'\", 'envoyer', 'aux', 'équipes', 'de', 'montage', 'du', 'movie', '48', 'heures', 'plus', 'tard', 'vous', 'recevez', 'votre', 'vidéo', 'montée', 'si', 'cette', 'solution', 'vous', 'intéresse', \"n'\", 'hésitez', 'pas', 'à', 'nous', 'contacter', 'à', 'bientôt', '[', 'musique', ']']\n['', 'bonjour', 'je', 'suis', 'louis', 'julian', 'ancien', 'de', \"sup'\", 'biotech', 'diplômé', \"d'\", 'ingénieur', 'en', 'biotechnologie', 'suite', 'à', 'ce', 'biotech', \"j'\", 'étais', 'embauché', 'comme', 'directrice', 'du', 'sinn', 'chez', 'bio', 'group', 'une', 'société', 'qui', 'faisait', 'du', 'coup', 'mouchard', 'et', 'maintenant', 'donc', 'je', 'travaille', 'avec', 'michel', 'des', 'portes', 'était', 'aux', 'manettes', 'à', 'construire', 'une', 'société', 'qui', \"s'\", 'appelle', 'aéro', 'mat', \"l'\", 'idée', \"d'\", 'aero', 'maths', \"c'\", 'est', 'de', 'cultiver', 'des', 'plantes', 'aromatiques', 'sur', 'les', 'toits', 'de', 'paris', 'à', 'destination', 'des', 'parisiens', 'donc', 'directement', 'dans', 'le', 'bâtiment', 'ou', 'dans', 'le', 'quartier', 'à', 'travers', 'le', 'réseau', \"d'\", 'amap', 'la', 'ruche', 'qui', 'dit', 'oui', 'et', \"d'\", 'autres', 'réseaux', 'de', 'distribution', 'on', 'cultive', 'sur', 'les', 'toits', 'pour', 'deux', 'raisons', \"d'\", 'abord', 'parce', \"qu'\", 'on', 'a', 'beaucoup', 'de', 'mal', 'à', 'trouver', 'dans', 'paris', 'des', 'herbes', 'aromatiques', 'fraîches', 'et', 'goûtu', 'et', 'donc', \"l'\", 'idée', \"d'\", 'ailleurs', 'aux', 'maths', \"c'\", 'est', 'vraiment', 'de', 'cultiver', 'sur', 'le', 'toit', 'pour', 'pouvoir', 'récolter', 'le', 'jour', 'de', 'la', 'consommation', 'et', 'avoir', 'quelque', 'chose', 'de', 'très', 'frais', 'et', 'la', 'seconde', 'raison', \"c'\", 'est', \"d'\", 'apporter', 'un', 'peu', 'de', 'biodiversité', 'dans', 'paris', 'on', 'apporte', 'de', 'la', 'verdure', 'et', 'donc', 'on', 'va', 'voir', 'toute', 'une', 'faune', 'qui', 'vient', 'sur', 'le', 'toit', 'et', \"c'\", 'est', 'super', 'chez', 'aéro', 'maths', 'on', 'cultive', 'en', 'hydroponie', 'écologique', \"l'\", 'idée', 'du', \"l'\", 'hydroponie', \"c'\", 'est', 'de', 'faire', 'pousser', 'les', 'plantes', 'sans', 'terre', 'directement', 'dans', \"l'\", 'eau', 'on', 'devient', 'asperges', 'et', \"l'\", 'eau', 'qui', 'contient', 'des', 'sels', 'minéraux', 'et', 'donc', 'les', 'nutriments', 'nécessaires', 'pour', 'la', 'plante', 'directement', 'à', 'la', 'racine', 'la', 'plante', 'va', 'pouvoir', 'absorber', 'à', 'ses', 'minéraux', 'et', 'pousser', \"l'\", 'eau', 'ensuite', 'va', 'retomber', 'dans', 'le', 'bac', 'initiale', 'ce', 'qui', 'à', 'faire', 'un', 'circuit', 'fermé', 'et', 'donc', 'ça', 'va', 'permettre', \"d'\", 'économiser', '90', '%', \"d'\", 'eau', 'par', 'rapport', 'à', 'une', 'culture', 'en', 'terre', 'le', 'second', 'intérêt', 'de', \"l'\", 'hydroponie', \"c'\", 'est', 'que', \"c'\", 'est', 'une', 'technique', 'qui', 'est', 'très', 'légère', 'on', 'est', 'à', 'moins', 'de', '100', 'kg', 'par', 'mètre', 'carré', 'faut', 'savoir', 'que', 'les', 'toitures', 'terrasses', 'parisiennes', 'ont', 'en', 'moyenne', '200', 'kg', 'par', 'mètre', 'carré', 'de', 'portance', 'et', 'donc', 'ça', 'permet', 'vraiment', 'de', \"s'\", 'adapter', 'à', 'tous', 'les', 'toits', 'terrasses', 'parisien', 'les', 'la', 'plupart', 'des', 'agriculteurs', 'urbains', 'ont', 'tendance', 'à', 'monter', 'des', 'bacs', 'en', 'terre', 'pour', 'cultiver', 'sur', 'les', 'toits', 'le', 'problème', 'de', 'la', 'terre', \"c'\", 'est', \"qu'\", 'on', 'va', 'très', 'vite', 'atteindre', 'déportance', 'supérieure', 'à', '7', '200', 'kg', 'par', 'mètre', 'carré', 'on', 'va', 'être', 'à', '300', 'voire', '500', 'kg', 'intérêts', 'et', 'donc', 'on', 'peut', 'pas', \"s'\", 'installer', 'partout', 'contrairement', 'à', \"l'\", 'hydroponie', 'qui', 'va', 'vraiment', 'pouvoir', \"s'\", 'adapter', 'aux', 'toitures', 'terrasses', 'parisiennes', 'dans', 'cette', 'belle', 'aventure', \"sup'\", 'biotech', 'est', 'toujours', 'là', 'pour', 'nous', 'aider', 'ça', \"c'\", 'est', 'vraiment', 'quelque', 'chose', 'de', 'top', 'on', 'a', 'un', 'gars', 'qui', 'va', 'nous', 'aider', 'pour', 'la', 'com', 'on', 'a', 'aussi', 'un', 'professeur', \"qu'\", 'avait', 'michel', 'en', 'marketing', 'qui', 'nous', 'propose', 'son', 'aide', 'et', 'il', 'ya', 'tout', 'un', 'réseau', 'qui', \"s'\", 'est', 'mis', 'en', 'place', 'ça', 'nous', 'a', 'vraiment', 'beaucoup', 'aidé', 'on', \"n'\", 'a', 'pas', 'ça', 'on', 'est', 'entrés', 'dans', 'le', 'réseau', 'ionis', '361', 'qui', 'est', \"l'\", 'incubateur', 'aussi', 'de', 'ionis', 'grâce', 'à', 'supiot', 'tech', 'donc', 'vraiment', 'là', 'ça', 'nous', 'a', 'beaucoup', 'apportés', 'pour', 'le', 'lancement', \"d'\", 'aero', 'matin', 'après', 'quelques', 'conseils', 'je', 'donnerai', 'aux', 'gens', 'qui', 'veulent', 'se', 'lancer', \"c'\", 'est', 'vraiment', 'ne', 'pas', 'lâcher', 'et', 'de', 'soumettre', 'à', 'plusieurs', 'parce', 'que', 'ça', 'aide', 'énormément', \"d'\", 'être', 'au', 'moins', 'deux', 'ou', 'trois', 'sur', 'un', 'projet', '[', 'musique', ']']\n['', 'on', 'peut', 'y', 'aller', 'là', 'le', 'cadre', 'il', 'est', 'bon', 'voilà', 'je', 'crois', \"c'\", 'est', 'pas', 'mal', 'allez', \"c'\", 'est', 'parti', 'pour', 'gagner', 'en', 'autonomie', 'en', 'réactivité', 'en', 'rapidité', 'on', \"s'\", 'est', 'doté', \"d'\", 'une', 'solution', 'pour', 'des', 'vidéos', 'made', 'in', 'eovi', 'mcd', 'limouzy', 'on', 'a', 'suivi', 'une', 'formation', 'poussée', 'cadrage', 'lumières', 'sons', 'on', 'sait', 'maintenant', 'mettre', 'en', 'valeur', 'vos', 'projets', 'et', 'vos', 'réussites', 'nous', 'irons', 'également', 'sur', 'le', 'terrain', 'à', 'la', 'rencontre', 'de', 'nos', 'adhérents', 'de', 'nos', 'partenaires', 'pour', 'les', 'mettre', 'en', 'boîte', 'enfin', 'on', 'va', 'les', 'contrer', 'préparez', 'vous', 'on', 'arrive', 'à', 'action', '[', 'musique', ']']\n['', \"c'\", 'était', 'global', 'avec', '38', 'bureaux', 'implantés', 'partout', 'dans', 'le', 'monde', 'mais', 'halak', 'en', 'groupe', 'et', 'pas', 'uniquement', 'à', 'paris', 'nous', 'étions', 'à', 'la', 'recherche', \"d'\", 'un', 'outil', 'capable', \"d'\", 'harmoniser', \"l'\", 'ensemble', 'des', 'vidéos', 'produites', 'par', 'les', 'collaborateurs', 'du', 'groupe', 'avec', 'les', 'dinos', 'vie', 'on', 'donne', 'un', 'cadre', 'et', 'une', 'simplicité', 'à', \"l'\", 'ensemble', 'de', 'toute', 'cette', 'production', 'de', 'vidéos', 'nous', 'avons', 'éliminé', 'toutes', 'les', 'problématiques', 'de', 'charte', 'que', \"l'\", 'on', 'soit', 'maintenant', 'à', 'paris', 'singapour', 'ou', 'à', 'new', 'york', 'toutes', 'les', 'vidéos', 'sont', 'aux', 'couleurs', 'de', 'score', 'mais', \"qu'\", 'ils', 'imovie', 'ont', 'produit', 'de', 'plus', 'en', 'plus', 'de', 'vidéos', 'parce', 'que', 'les', 'gens', 'sont', 'libres', 'de', 'leurs', 'mouvements', 'ils', 'sont', 'plus', 'autonomes', 'ils', 'peuvent', 'se', 'déplacer', 'avec', 'leur', 'smartphone', 'sans', 'utiliser', 'une', 'grosse', 'caméra', 'qui', 'peuvent', 'filmer', 'où', 'ils', 'veulent', 'et', 'ils', 'se', 'sont', 'beaucoup', 'plus', 'ce', 'format', 'court', 'et', 'dynamique', 'de', 'vidéos', 'correspond', 'à', 'la', 'culture', 'des', 'réseaux', 'sociaux', 'que', \"l'\", 'on', 'essaie', \"d'\", 'insuffler', 'chez', 'scor', 'on', 'espère', 'continuer', 'dans', 'cette', 'dynamique', 'et', 'cette', 'culture', 'de', 'la', 'vidéo', 'et', 'produire', 'toujours', 'plus', 'de', 'vidéos', 'avec', 'les', 'immoler']\n['', '[', 'musique', ']', 'bonjour', 'je', 'suis', 'séverine', 'du', 'service', 'communication', 'de', 'saint', 'gobain', 'distribution', 'bâtiment', 'france', 'et', 'je', 'vais', 'vous', 'présenter', 'izimmo', 'ville', 'vous', 'souhaitez', 'communiquer', 'avec', 'des', 'vidéos', 'nous', 'vous', 'proposons', 'la', 'solution', 'easy', 'movie', 'pour', 'vous', 'y', 'aider', 'simple', 'pratique', 'et', 'rapide', \"l'\", 'outil', 'imovie', 'permet', 'de', 'faire', 'des', 'vidéos', 'courtes', 'à', 'moindre', 'coût', 'et', 'de', 'qualité', 'comment', 'fonctionne', 'izimmo', 'vient', 'créer', 'un', 'nouveau', 'projet', 'sélectionnez', 'votre', 'scénario', 'choisissez', 'votre', 'plan', 'est', 'filmée', 'pour', 'utiliser', 'imovie', \"c'\", 'est', 'très', 'simple', 'il', 'vous', 'suffit', 'de', 'suivre', 'les', 'instructions', 'sur', \"l'\", 'appli', 'imovie', 'et', 'de', 'lancer', \"l'\", 'enregistrement', 'une', 'fois', 'votre', 'film', 'terminé', 'il', 'vous', 'suffit', 'de', \"l'\", 'envoyer', 'aux', 'équipes', 'de', 'montage', 'du', 'movie', '48', 'heures', 'plus', 'tard', 'vous', 'recevez', 'votre', 'vidéo', 'montée', 'si', 'cette', 'solution', 'vous', 'intéresse', \"n'\", 'hésitez', 'pas', 'à', 'nous', 'contacter', 'à', 'bientôt', '[', 'musique', ']']\n['', 'bonjour', 'je', 'suis', 'louis', 'julian', 'ancien', 'de', \"sup'\", 'biotech', 'diplômé', \"d'\", 'ingénieur', 'en', 'biotechnologie', 'suite', 'à', 'ce', 'biotech', \"j'\", 'étais', 'embauché', 'comme', 'directrice', 'du', 'sinn', 'chez', 'bio', 'group', 'une', 'société', 'qui', 'faisait', 'du', 'coup', 'mouchard', 'et', 'maintenant', 'donc', 'je', 'travaille', 'avec', 'michel', 'des', 'portes', 'était', 'aux', 'manettes', 'à', 'construire', 'une', 'société', 'qui', \"s'\", 'appelle', 'aéro', 'mat', \"l'\", 'idée', \"d'\", 'aero', 'maths', \"c'\", 'est', 'de', 'cultiver', 'des', 'plantes', 'aromatiques', 'sur', 'les', 'toits', 'de', 'paris', 'à', 'destination', 'des', 'parisiens', 'donc', 'directement', 'dans', 'le', 'bâtiment', 'ou', 'dans', 'le', 'quartier', 'à', 'travers', 'le', 'réseau', \"d'\", 'amap', 'la', 'ruche', 'qui', 'dit', 'oui', 'et', \"d'\", 'autres', 'réseaux', 'de', 'distribution', 'on', 'cultive', 'sur', 'les', 'toits', 'pour', 'deux', 'raisons', \"d'\", 'abord', 'parce', \"qu'\", 'on', 'a', 'beaucoup', 'de', 'mal', 'à', 'trouver', 'dans', 'paris', 'des', 'herbes', 'aromatiques', 'fraîches', 'et', 'goûtu', 'et', 'donc', \"l'\", 'idée', \"d'\", 'ailleurs', 'aux', 'maths', \"c'\", 'est', 'vraiment', 'de', 'cultiver', 'sur', 'le', 'toit', 'pour', 'pouvoir', 'récolter', 'le', 'jour', 'de', 'la', 'consommation', 'et', 'avoir', 'quelque', 'chose', 'de', 'très', 'frais', 'et', 'la', 'seconde', 'raison', \"c'\", 'est', \"d'\", 'apporter', 'un', 'peu', 'de', 'biodiversité', 'dans', 'paris', 'on', 'apporte', 'de', 'la', 'verdure', 'et', 'donc', 'on', 'va', 'voir', 'toute', 'une', 'faune', 'qui', 'vient', 'sur', 'le', 'toit', 'et', \"c'\", 'est', 'super', 'chez', 'aéro', 'maths', 'on', 'cultive', 'en', 'hydroponie', 'écologique', \"l'\", 'idée', 'du', \"l'\", 'hydroponie', \"c'\", 'est', 'de', 'faire', 'pousser', 'les', 'plantes', 'sans', 'terre', 'directement', 'dans', \"l'\", 'eau', 'on', 'devient', 'asperges', 'et', \"l'\", 'eau', 'qui', 'contient', 'des', 'sels', 'minéraux', 'et', 'donc', 'les', 'nutriments', 'nécessaires', 'pour', 'la', 'plante', 'directement', 'à', 'la', 'racine', 'la', 'plante', 'va', 'pouvoir', 'absorber', 'à', 'ses', 'minéraux', 'et', 'pousser', \"l'\", 'eau', 'ensuite', 'va', 'retomber', 'dans', 'le', 'bac', 'initiale', 'ce', 'qui', 'à', 'faire', 'un', 'circuit', 'fermé', 'et', 'donc', 'ça', 'va', 'permettre', \"d'\", 'économiser', '90', '%', \"d'\", 'eau', 'par', 'rapport', 'à', 'une', 'culture', 'en', 'terre', 'le', 'second', 'intérêt', 'de', \"l'\", 'hydroponie', \"c'\", 'est', 'que', \"c'\", 'est', 'une', 'technique', 'qui', 'est', 'très', 'légère', 'on', 'est', 'à', 'moins', 'de', '100', 'kg', 'par', 'mètre', 'carré', 'faut', 'savoir', 'que', 'les', 'toitures', 'terrasses', 'parisiennes', 'ont', 'en', 'moyenne', '200', 'kg', 'par', 'mètre', 'carré', 'de', 'portance', 'et', 'donc', 'ça', 'permet', 'vraiment', 'de', \"s'\", 'adapter', 'à', 'tous', 'les', 'toits', 'terrasses', 'parisien', 'les', 'la', 'plupart', 'des', 'agriculteurs', 'urbains', 'ont', 'tendance', 'à', 'monter', 'des', 'bacs', 'en', 'terre', 'pour', 'cultiver', 'sur', 'les', 'toits', 'le', 'problème', 'de', 'la', 'terre', \"c'\", 'est', \"qu'\", 'on', 'va', 'très', 'vite', 'atteindre', 'déportance', 'supérieure', 'à', '7', '200', 'kg', 'par', 'mètre', 'carré', 'on', 'va', 'être', 'à', '300', 'voire', '500', 'kg', 'intérêts', 'et', 'donc', 'on', 'peut', 'pas', \"s'\", 'installer', 'partout', 'contrairement', 'à', \"l'\", 'hydroponie', 'qui', 'va', 'vraiment', 'pouvoir', \"s'\", 'adapter', 'aux', 'toitures', 'terrasses', 'parisiennes', 'dans', 'cette', 'belle', 'aventure', \"sup'\", 'biotech', 'est', 'toujours', 'là', 'pour', 'nous', 'aider', 'ça', \"c'\", 'est', 'vraiment', 'quelque', 'chose', 'de', 'top', 'on', 'a', 'un', 'gars', 'qui', 'va', 'nous', 'aider', 'pour', 'la', 'com', 'on', 'a', 'aussi', 'un', 'professeur', \"qu'\", 'avait', 'michel', 'en', 'marketing', 'qui', 'nous', 'propose', 'son', 'aide', 'et', 'il', 'ya', 'tout', 'un', 'réseau', 'qui', \"s'\", 'est', 'mis', 'en', 'place', 'ça', 'nous', 'a', 'vraiment', 'beaucoup', 'aidé', 'on', \"n'\", 'a', 'pas', 'ça', 'on', 'est', 'entrés', 'dans', 'le', 'réseau', 'ionis', '361', 'qui', 'est', \"l'\", 'incubateur', 'aussi', 'de', 'ionis', 'grâce', 'à', 'supiot', 'tech', 'donc', 'vraiment', 'là', 'ça', 'nous', 'a', 'beaucoup', 'apportés', 'pour', 'le', 'lancement', \"d'\", 'aero', 'matin', 'après', 'quelques', 'conseils', 'je', 'donnerai', 'aux', 'gens', 'qui', 'veulent', 'se', 'lancer', \"c'\", 'est', 'vraiment', 'ne', 'pas', 'lâcher', 'et', 'de', 'soumettre', 'à', 'plusieurs', 'parce', 'que', 'ça', 'aide', 'énormément', \"d'\", 'être', 'au', 'moins', 'deux', 'ou', 'trois', 'sur', 'un', 'projet', '[', 'musique', ']']\n['', 'on', 'peut', 'y', 'aller', 'là', 'le', 'cadre', 'il', 'est', 'bon', 'voilà', 'je', 'crois', \"c'\", 'est', 'pas', 'mal', 'allez', \"c'\", 'est', 'parti', 'pour', 'gagner', 'en', 'autonomie', 'en', 'réactivité', 'en', 'rapidité', 'on', \"s'\", 'est', 'doté', \"d'\", 'une', 'solution', 'pour', 'des', 'vidéos', 'made', 'in', 'eovi', 'mcd', 'limouzy', 'on', 'a', 'suivi', 'une', 'formation', 'poussée', 'cadrage', 'lumières', 'sons', 'on', 'sait', 'maintenant', 'mettre', 'en', 'valeur', 'vos', 'projets', 'et', 'vos', 'réussites', 'nous', 'irons', 'également', 'sur', 'le', 'terrain', 'à', 'la', 'rencontre', 'de', 'nos', 'adhérents', 'de', 'nos', 'partenaires', 'pour', 'les', 'mettre', 'en', 'boîte', 'enfin', 'on', 'va', 'les', 'contrer', 'préparez', 'vous', 'on', 'arrive', 'à', 'action', '[', 'musique', ']']\n['', \"c'\", 'était', 'global', 'avec', '38', 'bureaux', 'implantés', 'partout', 'dans', 'le', 'monde', 'mais', 'halak', 'en', 'groupe', 'et', 'pas', 'uniquement', 'à', 'paris', 'nous', 'étions', 'à', 'la', 'recherche', \"d'\", 'un', 'outil', 'capable', \"d'\", 'harmoniser', \"l'\", 'ensemble', 'des', 'vidéos', 'produites', 'par', 'les', 'collaborateurs', 'du', 'groupe', 'avec', 'les', 'dinos', 'vie', 'on', 'donne', 'un', 'cadre', 'et', 'une', 'simplicité', 'à', \"l'\", 'ensemble', 'de', 'toute', 'cette', 'production', 'de', 'vidéos', 'nous', 'avons', 'éliminé', 'toutes', 'les', 'problématiques', 'de', 'charte', 'que', \"l'\", 'on', 'soit', 'maintenant', 'à', 'paris', 'singapour', 'ou', 'à', 'new', 'york', 'toutes', 'les', 'vidéos', 'sont', 'aux', 'couleurs', 'de', 'score', 'mais', \"qu'\", 'ils', 'imovie', 'ont', 'produit', 'de', 'plus', 'en', 'plus', 'de', 'vidéos', 'parce', 'que', 'les', 'gens', 'sont', 'libres', 'de', 'leurs', 'mouvements', 'ils', 'sont', 'plus', 'autonomes', 'ils', 'peuvent', 'se', 'déplacer', 'avec', 'leur', 'smartphone', 'sans', 'utiliser', 'une', 'grosse', 'caméra', 'qui', 'peuvent', 'filmer', 'où', 'ils', 'veulent', 'et', 'ils', 'se', 'sont', 'beaucoup', 'plus', 'ce', 'format', 'court', 'et', 'dynamique', 'de', 'vidéos', 'correspond', 'à', 'la', 'culture', 'des', 'réseaux', 'sociaux', 'que', \"l'\", 'on', 'essaie', \"d'\", 'insuffler', 'chez', 'scor', 'on', 'espère', 'continuer', 'dans', 'cette', 'dynamique', 'et', 'cette', 'culture', 'de', 'la', 'vidéo', 'et', 'produire', 'toujours', 'plus', 'de', 'vidéos', 'avec', 'les', 'immoler']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "easy_lda = LatentDirichletAllocation(n_components=5)\n",
    "X = df['texte']\n",
    "easy_pipeline = Pipeline([('vectorizer', easy_vectorizer),(\"topic-analyser\", easy_lda)])\n",
    "easy_pipeline.fit(X=X)\n",
    "topic_analysis = easy_pipeline.transform(X=X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['', '[', 'musique', ']', 'bonjour', 'je', 'suis', 'séverine', 'du', 'service', 'communication', 'de', 'saint', 'gobain', 'distribution', 'bâtiment', 'france', 'et', 'je', 'vais', 'vous', 'présenter', 'izimmo', 'ville', 'vous', 'souhaitez', 'communiquer', 'avec', 'des', 'vidéos', 'nous', 'vous', 'proposons', 'la', 'solution', 'easy', 'movie', 'pour', 'vous', 'y', 'aider', 'simple', 'pratique', 'et', 'rapide', \"l'\", 'outil', 'imovie', 'permet', 'de', 'faire', 'des', 'vidéos', 'courtes', 'à', 'moindre', 'coût', 'et', 'de', 'qualité', 'comment', 'fonctionne', 'izimmo', 'vient', 'créer', 'un', 'nouveau', 'projet', 'sélectionnez', 'votre', 'scénario', 'choisissez', 'votre', 'plan', 'est', 'filmée', 'pour', 'utiliser', 'imovie', \"c'\", 'est', 'très', 'simple', 'il', 'vous', 'suffit', 'de', 'suivre', 'les', 'instructions', 'sur', \"l'\", 'appli', 'imovie', 'et', 'de', 'lancer', \"l'\", 'enregistrement', 'une', 'fois', 'votre', 'film', 'terminé', 'il', 'vous', 'suffit', 'de', \"l'\", 'envoyer', 'aux', 'équipes', 'de', 'montage', 'du', 'movie', '48', 'heures', 'plus', 'tard', 'vous', 'recevez', 'votre', 'vidéo', 'montée', 'si', 'cette', 'solution', 'vous', 'intéresse', \"n'\", 'hésitez', 'pas', 'à', 'nous', 'contacter', 'à', 'bientôt', '[', 'musique', ']']\n['', 'bonjour', 'je', 'suis', 'louis', 'julian', 'ancien', 'de', \"sup'\", 'biotech', 'diplômé', \"d'\", 'ingénieur', 'en', 'biotechnologie', 'suite', 'à', 'ce', 'biotech', \"j'\", 'étais', 'embauché', 'comme', 'directrice', 'du', 'sinn', 'chez', 'bio', 'group', 'une', 'société', 'qui', 'faisait', 'du', 'coup', 'mouchard', 'et', 'maintenant', 'donc', 'je', 'travaille', 'avec', 'michel', 'des', 'portes', 'était', 'aux', 'manettes', 'à', 'construire', 'une', 'société', 'qui', \"s'\", 'appelle', 'aéro', 'mat', \"l'\", 'idée', \"d'\", 'aero', 'maths', \"c'\", 'est', 'de', 'cultiver', 'des', 'plantes', 'aromatiques', 'sur', 'les', 'toits', 'de', 'paris', 'à', 'destination', 'des', 'parisiens', 'donc', 'directement', 'dans', 'le', 'bâtiment', 'ou', 'dans', 'le', 'quartier', 'à', 'travers', 'le', 'réseau', \"d'\", 'amap', 'la', 'ruche', 'qui', 'dit', 'oui', 'et', \"d'\", 'autres', 'réseaux', 'de', 'distribution', 'on', 'cultive', 'sur', 'les', 'toits', 'pour', 'deux', 'raisons', \"d'\", 'abord', 'parce', \"qu'\", 'on', 'a', 'beaucoup', 'de', 'mal', 'à', 'trouver', 'dans', 'paris', 'des', 'herbes', 'aromatiques', 'fraîches', 'et', 'goûtu', 'et', 'donc', \"l'\", 'idée', \"d'\", 'ailleurs', 'aux', 'maths', \"c'\", 'est', 'vraiment', 'de', 'cultiver', 'sur', 'le', 'toit', 'pour', 'pouvoir', 'récolter', 'le', 'jour', 'de', 'la', 'consommation', 'et', 'avoir', 'quelque', 'chose', 'de', 'très', 'frais', 'et', 'la', 'seconde', 'raison', \"c'\", 'est', \"d'\", 'apporter', 'un', 'peu', 'de', 'biodiversité', 'dans', 'paris', 'on', 'apporte', 'de', 'la', 'verdure', 'et', 'donc', 'on', 'va', 'voir', 'toute', 'une', 'faune', 'qui', 'vient', 'sur', 'le', 'toit', 'et', \"c'\", 'est', 'super', 'chez', 'aéro', 'maths', 'on', 'cultive', 'en', 'hydroponie', 'écologique', \"l'\", 'idée', 'du', \"l'\", 'hydroponie', \"c'\", 'est', 'de', 'faire', 'pousser', 'les', 'plantes', 'sans', 'terre', 'directement', 'dans', \"l'\", 'eau', 'on', 'devient', 'asperges', 'et', \"l'\", 'eau', 'qui', 'contient', 'des', 'sels', 'minéraux', 'et', 'donc', 'les', 'nutriments', 'nécessaires', 'pour', 'la', 'plante', 'directement', 'à', 'la', 'racine', 'la', 'plante', 'va', 'pouvoir', 'absorber', 'à', 'ses', 'minéraux', 'et', 'pousser', \"l'\", 'eau', 'ensuite', 'va', 'retomber', 'dans', 'le', 'bac', 'initiale', 'ce', 'qui', 'à', 'faire', 'un', 'circuit', 'fermé', 'et', 'donc', 'ça', 'va', 'permettre', \"d'\", 'économiser', '90', '%', \"d'\", 'eau', 'par', 'rapport', 'à', 'une', 'culture', 'en', 'terre', 'le', 'second', 'intérêt', 'de', \"l'\", 'hydroponie', \"c'\", 'est', 'que', \"c'\", 'est', 'une', 'technique', 'qui', 'est', 'très', 'légère', 'on', 'est', 'à', 'moins', 'de', '100', 'kg', 'par', 'mètre', 'carré', 'faut', 'savoir', 'que', 'les', 'toitures', 'terrasses', 'parisiennes', 'ont', 'en', 'moyenne', '200', 'kg', 'par', 'mètre', 'carré', 'de', 'portance', 'et', 'donc', 'ça', 'permet', 'vraiment', 'de', \"s'\", 'adapter', 'à', 'tous', 'les', 'toits', 'terrasses', 'parisien', 'les', 'la', 'plupart', 'des', 'agriculteurs', 'urbains', 'ont', 'tendance', 'à', 'monter', 'des', 'bacs', 'en', 'terre', 'pour', 'cultiver', 'sur', 'les', 'toits', 'le', 'problème', 'de', 'la', 'terre', \"c'\", 'est', \"qu'\", 'on', 'va', 'très', 'vite', 'atteindre', 'déportance', 'supérieure', 'à', '7', '200', 'kg', 'par', 'mètre', 'carré', 'on', 'va', 'être', 'à', '300', 'voire', '500', 'kg', 'intérêts', 'et', 'donc', 'on', 'peut', 'pas', \"s'\", 'installer', 'partout', 'contrairement', 'à', \"l'\", 'hydroponie', 'qui', 'va', 'vraiment', 'pouvoir', \"s'\", 'adapter', 'aux', 'toitures', 'terrasses', 'parisiennes', 'dans', 'cette', 'belle', 'aventure', \"sup'\", 'biotech', 'est', 'toujours', 'là', 'pour', 'nous', 'aider', 'ça', \"c'\", 'est', 'vraiment', 'quelque', 'chose', 'de', 'top', 'on', 'a', 'un', 'gars', 'qui', 'va', 'nous', 'aider', 'pour', 'la', 'com', 'on', 'a', 'aussi', 'un', 'professeur', \"qu'\", 'avait', 'michel', 'en', 'marketing', 'qui', 'nous', 'propose', 'son', 'aide', 'et', 'il', 'ya', 'tout', 'un', 'réseau', 'qui', \"s'\", 'est', 'mis', 'en', 'place', 'ça', 'nous', 'a', 'vraiment', 'beaucoup', 'aidé', 'on', \"n'\", 'a', 'pas', 'ça', 'on', 'est', 'entrés', 'dans', 'le', 'réseau', 'ionis', '361', 'qui', 'est', \"l'\", 'incubateur', 'aussi', 'de', 'ionis', 'grâce', 'à', 'supiot', 'tech', 'donc', 'vraiment', 'là', 'ça', 'nous', 'a', 'beaucoup', 'apportés', 'pour', 'le', 'lancement', \"d'\", 'aero', 'matin', 'après', 'quelques', 'conseils', 'je', 'donnerai', 'aux', 'gens', 'qui', 'veulent', 'se', 'lancer', \"c'\", 'est', 'vraiment', 'ne', 'pas', 'lâcher', 'et', 'de', 'soumettre', 'à', 'plusieurs', 'parce', 'que', 'ça', 'aide', 'énormément', \"d'\", 'être', 'au', 'moins', 'deux', 'ou', 'trois', 'sur', 'un', 'projet', '[', 'musique', ']']\n['', 'on', 'peut', 'y', 'aller', 'là', 'le', 'cadre', 'il', 'est', 'bon', 'voilà', 'je', 'crois', \"c'\", 'est', 'pas', 'mal', 'allez', \"c'\", 'est', 'parti', 'pour', 'gagner', 'en', 'autonomie', 'en', 'réactivité', 'en', 'rapidité', 'on', \"s'\", 'est', 'doté', \"d'\", 'une', 'solution', 'pour', 'des', 'vidéos', 'made', 'in', 'eovi', 'mcd', 'limouzy', 'on', 'a', 'suivi', 'une', 'formation', 'poussée', 'cadrage', 'lumières', 'sons', 'on', 'sait', 'maintenant', 'mettre', 'en', 'valeur', 'vos', 'projets', 'et', 'vos', 'réussites', 'nous', 'irons', 'également', 'sur', 'le', 'terrain', 'à', 'la', 'rencontre', 'de', 'nos', 'adhérents', 'de', 'nos', 'partenaires', 'pour', 'les', 'mettre', 'en', 'boîte', 'enfin', 'on', 'va', 'les', 'contrer', 'préparez', 'vous', 'on', 'arrive', 'à', 'action', '[', 'musique', ']']\n['', \"c'\", 'était', 'global', 'avec', '38', 'bureaux', 'implantés', 'partout', 'dans', 'le', 'monde', 'mais', 'halak', 'en', 'groupe', 'et', 'pas', 'uniquement', 'à', 'paris', 'nous', 'étions', 'à', 'la', 'recherche', \"d'\", 'un', 'outil', 'capable', \"d'\", 'harmoniser', \"l'\", 'ensemble', 'des', 'vidéos', 'produites', 'par', 'les', 'collaborateurs', 'du', 'groupe', 'avec', 'les', 'dinos', 'vie', 'on', 'donne', 'un', 'cadre', 'et', 'une', 'simplicité', 'à', \"l'\", 'ensemble', 'de', 'toute', 'cette', 'production', 'de', 'vidéos', 'nous', 'avons', 'éliminé', 'toutes', 'les', 'problématiques', 'de', 'charte', 'que', \"l'\", 'on', 'soit', 'maintenant', 'à', 'paris', 'singapour', 'ou', 'à', 'new', 'york', 'toutes', 'les', 'vidéos', 'sont', 'aux', 'couleurs', 'de', 'score', 'mais', \"qu'\", 'ils', 'imovie', 'ont', 'produit', 'de', 'plus', 'en', 'plus', 'de', 'vidéos', 'parce', 'que', 'les', 'gens', 'sont', 'libres', 'de', 'leurs', 'mouvements', 'ils', 'sont', 'plus', 'autonomes', 'ils', 'peuvent', 'se', 'déplacer', 'avec', 'leur', 'smartphone', 'sans', 'utiliser', 'une', 'grosse', 'caméra', 'qui', 'peuvent', 'filmer', 'où', 'ils', 'veulent', 'et', 'ils', 'se', 'sont', 'beaucoup', 'plus', 'ce', 'format', 'court', 'et', 'dynamique', 'de', 'vidéos', 'correspond', 'à', 'la', 'culture', 'des', 'réseaux', 'sociaux', 'que', \"l'\", 'on', 'essaie', \"d'\", 'insuffler', 'chez', 'scor', 'on', 'espère', 'continuer', 'dans', 'cette', 'dynamique', 'et', 'cette', 'culture', 'de', 'la', 'vidéo', 'et', 'produire', 'toujours', 'plus', 'de', 'vidéos', 'avec', 'les', 'immoler']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "extractTopicAndLoadInDataframe(count_vect=easy_vectorizer,dataFrame=df, lda=easy_lda, nb_words=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 title  \\\n0                      saint-gobin.srt   \n1                       supbiotect.srt   \n2  evo-comment-utiliser-easy-movie.srt   \n3       scor-simplifier-harmoniser.srt   \n\n                                               texte  \\\n0   [Musique] bonjour je suis séverine du service...   \n1   bonjour je suis louis julian ancien de sup'bi...   \n2   on peut y aller là le cadre il est bon voilà ...   \n3   c'était global avec 38 bureaux implantés part...   \n\n                                           labels  \n0               [solution, musique, ], [, imovie]  \n1       [toits, terre, hydroponie, eau, vraiment]  \n2                 [préparez, sons, , mal, mettre]  \n3  [dynamique, ensemble, groupe, culture, vidéos]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>texte</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>saint-gobin.srt</td>\n      <td>[Musique] bonjour je suis séverine du service...</td>\n      <td>[solution, musique, ], [, imovie]</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>supbiotect.srt</td>\n      <td>bonjour je suis louis julian ancien de sup'bi...</td>\n      <td>[toits, terre, hydroponie, eau, vraiment]</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>evo-comment-utiliser-easy-movie.srt</td>\n      <td>on peut y aller là le cadre il est bon voilà ...</td>\n      <td>[préparez, sons, , mal, mettre]</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>scor-simplifier-harmoniser.srt</td>\n      <td>c'était global avec 38 bureaux implantés part...</td>\n      <td>[dynamique, ensemble, groupe, culture, vidéos]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 59
    }
   ],
   "source": [
    "df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}