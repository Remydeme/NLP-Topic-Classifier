{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Determine the main topics covered in a video with a machine learning model. It is best to extract tags or keywords from these videos. For example, for a video about politics and agriculture, two different topics are discussed. Our machine learning algorithm will have to be able to extract the words related to these two topics and it will have to be able in the end to say to which topic the video is the most strongly associated. In fact, the video may contain vocabulary words related to agriculture, but if the main subject is politics, the video will have to be classified in the political category with a higher probability.\n",
    "\n",
    "We thus obtain after application of the model the words related to each topic (words that can serve as keys) and our video has been labeled.\n",
    "\n",
    "It was also mentioned the possibility of doing sentiment analysis, to advise the user in the editing of his video. The detection of a video \"fun\"may allow to advise the user the use of certain theme, music or color.\n",
    "\n",
    "Questions :\n",
    "\n",
    "1- How to determine the different topics?\n",
    "2- What is the maximum number of topics in a text that should be considered?\n",
    "3- How to determine the maximum number of topics that our model must learn to recognize?\n",
    "\n",
    "\n",
    "Comment déterminer les différentes topiques ?\n",
    "\n",
    "In order to solve this problem, we proposed to use the Derichlet Latent Allocation algorithm. This algorithm for a given K of desired topics will analyze a corpus of texts and extract K topical. He will associate with each topic the words with the highest probability of belonging to the topic ((P (W | K)). It will be important to determine the good K.\n",
    "\n",
    "The LDA output K set of words. It does not give the name of the topic associated with each set. How to draw this set of words? We will have to do it manually using a graphical interface.\n",
    " \n",
    "Label the text with a single topic\n",
    "\n",
    "We thought that it would be interesting to use the extracted keywords for each of the topics in order to lead a second model. This model will learn to associate a set of words with a single topic. At the output of the model, we obtain for K topical K probabilities. Thus, it will be possible to select N topics among the K found. Those with a probability greater than a threshold. \n",
    "\n",
    "\n",
    "In order to classify texts by topical 5 model were retained:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Scikit-learn:\n",
    "\n",
    "* Linear-SVR\n",
    "* KNN\n",
    "Logistic regression\n",
    "XGBoost\n",
    "\n",
    "Deep-learning (with Pytorch) :\n",
    "\n",
    "* Classification with attention based network (with BLSTM).\n",
    "\n",
    "\n",
    " \n",
    " Comparaison de model :\n",
    " https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568 \n",
    " \n",
    " Attention networks:\n",
    " https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/\n",
    " https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c\n",
    "      "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition  import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd \n",
    "import pysrt\n",
    "\n",
    "class LoadData:\n",
    "    __df = pd.DataFrame()\n",
    "\n",
    "    def __init__(self, path, extension='srt'):\n",
    "        self.path = path \n",
    "        self.__files = [f for f in listdir(self.path) if (isfile(join(self.path, f)) and f.endswith(extension))]\n",
    "    \n",
    "    def files(self):\n",
    "        return self.__files\n",
    "    \n",
    "    def data(self, verbose=True):\n",
    "        textes = []\n",
    "        for file in self.__files:\n",
    "            if verbose :\n",
    "                print(f\"reading info of file {file}\")\n",
    "            texte = self.__treat_srt_file(file, verbose=verbose)\n",
    "            textes.append(texte)\n",
    "        self.__df['title'] = self.files()\n",
    "        self.__df['texte'] = textes\n",
    "        return self.__df\n",
    "        \n",
    "    def __treat_srt_file(self, file, verbose):\n",
    "        f_path = \"{}{}\".format(self.path, file)\n",
    "        print(f_path)\n",
    "        f = pysrt.open(f_path, encoding='utf-8')\n",
    "        article = \"\"\n",
    "        for t in f:\n",
    "            article += \" \" + t.text\n",
    "        print(f\"article {article}\")\n",
    "        return article\n",
    "            \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "supbiotect.srt\nevo-comment-utiliser-easy-movie.srt\nscor-simplifier-harmoniser.srt\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loader = LoadData(path='./TextFiles')\n",
    "\n",
    "for file in loader.files():\n",
    "    print(file)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "reading info of file supbiotect.srt\n./TextFilessupbiotect.srt\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d72a848bdca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b1425371a4d7>\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"reading info of file {file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtexte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__treat_srt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtextes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b1425371a4d7>\u001b[0m in \u001b[0;36m__treat_srt_file\u001b[0;34m(self, file, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/remy.d.w/.virtualenvs/PROROK/untitled/lib/python3.7/site-packages/pysrt/srtfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, path, encoding, error_handling)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mcontain\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbit\u001b[0m \u001b[0morder\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munless\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mutf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_unicode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaimed_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mnew_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mnew_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_handling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_handling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/remy.d.w/.virtualenvs/PROROK/untitled/lib/python3.7/site-packages/pysrt/srtfile.py\u001b[0m in \u001b[0;36m_open_unicode_file\u001b[0;34m(cls, path, claimed_encoding)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_unicode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaimed_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclaimed_encoding\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0msource_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# get rid of BOM if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './TextFilessupbiotect.srt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './TextFilessupbiotect.srt'",
     "output_type": "error"
    }
   ],
   "source": [
    "df = loader.data()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CountVectorizer:\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature \n",
    "selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "* max_df : If a word appear in more than 90% of the document it will be discarded \n",
    "* min_df : If a word appear less than 2 time it will not be take in count \n",
    "\n",
    "I used spacy french stop words list. We need to remove all the french \n",
    "stop word from our token because they appear in all the document and \n",
    "can be considered as topic's words. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "import spacy.cli\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "spacy_nlp = spacy.load('fr_core_news_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spacy_stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "for i, w in enumerate(spacy_stop_words):\n",
    "    print(w)\n",
    "    if i >= 50:\n",
    "        break "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=spacy_stop_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtm = count_vectorizer.fit_transform(df['texte'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LDA use the naives bayes probability to affect each word to a \n",
    "topic and text to a topic.\n",
    "\n",
    "* each word is associated to a topic. This topic is the one that have the \n",
    "best chance to  generate the word in this document. Example the word 'formula one'\n",
    " have higher probabilty to be generated in a sport or car document that in a cooking one.\n",
    " \n",
    "* The new theme is the one that have the best chance to generate the document.\n",
    "\n",
    "* P(T | D) give the likelihood that the Doc D is affected to the Theme T  \n",
    "* P(W | T) give the likelihood that the Word W is affected to the Theme T\n",
    "\n",
    "Here we want to find the 10 topic. For each topic a list of word are choosen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=10)\n",
    "\n",
    "LDA.fit(dtm)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    print(f\"Topic n° {i}\")\n",
    "    last_ten_words = topic.argsort()[-5:]\n",
    "    features = count_vectorizer.get_feature_names()\n",
    "    for index in last_ten_words:\n",
    "        print(features[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "supbiotech_vec = count_vectorizer.transform([df.iloc[0, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saint_gobin_topics = LDA.transform(supbiotech_vec)\n",
    "\n",
    "def extractTopic(components):\n",
    "    last_ten_words = components.argsort()[-5:]\n",
    "    features = count_vectorizer.get_feature_names()\n",
    "    for index in last_ten_words:\n",
    "        print(features[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = np.argmax(saint_gobin_topics)\n",
    "extractTopic(LDA.components_[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to add each labels array to his corresponding text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extractTopicAndLoadInDataframe(count_vect, dataFrame, lda, nb_words=5):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param cv: countVectorizer that was applied on the documents \n",
    "    :param df: the dataframe that contains the data \n",
    "    :param lda: Latent derichlet allocation trained \n",
    "    :param nb_words: number of words by topic that you want to used\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    document_labels = []\n",
    "    for i, text in enumerate(dataFrame['texte']):\n",
    "        text_vectorized = count_vect.transform([text])\n",
    "        index = np.argmax(lda.transform(text_vectorized))\n",
    "        topic = lda.components_[index]\n",
    "        topic_word_index = topic.argsort()[-nb_words:] # get the 5 best word of the topic\n",
    "        features = count_vect.get_feature_names()\n",
    "        labels = []\n",
    "        for j in topic_word_index:\n",
    "            labels.append(features[j]) \n",
    "        document_labels.append(labels)\n",
    "    dataFrame['labels'] = document_labels\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extractTopicAndLoadInDataframe(count_vect=count_vectorizer,dataFrame=df, lda=LDA, nb_words=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I build a pipline. This pipline contains:\n",
    " * the cleaning of the data\n",
    "   * lemming it's a normalization technique which converts high dimensional features into low dimensional features.\n",
    "       We simply identify prefix and suffix and produce the simplest version of the word. \n",
    "       For example : connect, connection, connected => have different grammatical function but for ML models we need\n",
    "       that this words be one simple word (connect) because it's can be confusing.\n",
    "       \n",
    "   * tokenizing : transform into token \n",
    "   * removing punct : It's useless for text classification \n",
    "   * removing stop words : stop words can lead to poor performance. \n",
    "  \n",
    " * Vectorize the data with CountVectorizer: Convert a collection of text documents to a matrix of token counts\n",
    " * Apply the LDA on the data : have been previously explained. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import spacy.cli\n",
    "from spacy.lang.fr import French\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "spacy_nlp = spacy.load('fr_core_news_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = French()\n",
    "\n",
    "tokenizer = French()\n",
    "\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "punctuations = spacy.lang.fr.punctuation.LIST_PUNCT\n",
    "\n",
    "def easy_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param sentence: the texte we want to treat with our parser \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # creating our token object \n",
    "    easy_tokens = parser(sentence)\n",
    "    \n",
    "    # lemming  and lowering. Stripping white space \n",
    "    # if the word is not a Pronoun lower the word and strip space else return the lower word. \n",
    "    # Because  spacy replace pronouns by \"-PRON-\" on lemming call. as we need the pronoun we can not do its lemmatization\n",
    "    \n",
    "    easy_tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in easy_tokens ]\n",
    "    \n",
    "    \n",
    "    # remove stop words \n",
    "    easy_tokens = [word  for word in easy_tokens if (word not in stop_words and word not in punctuations)]\n",
    "\n",
    "    \n",
    "    return easy_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "class PrepareText(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Take a dataframe as paremeter and fit to the data set è\n",
    "        :param X: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        remove stop word in the text and retore the text\n",
    "        return a data frame that contains all the text \n",
    "        :param X: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return [clean_text(text) for text in X]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "easy_vectorizer = CountVectorizer(tokenizer=easy_tokenizer, ngram_range=(1,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "easy_lda = LatentDirichletAllocation(n_components=5)\n",
    "X = df['texte']\n",
    "easy_pipeline = Pipeline([('vectorizer', easy_vectorizer),(\"topic-analyser\", easy_lda)])\n",
    "easy_pipeline.fit(X=X)\n",
    "topic_analysis = easy_pipeline.transform(X=X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extractTopicAndLoadInDataframe(count_vect=easy_vectorizer,dataFrame=df, lda=easy_lda, nb_words=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}